# Write your short answers in this file, replacing the placeholders as appropriate.
# This assignment consists of 1 parts for a total of 27 points.
# For numerical answers, copy and paste at least 5 significant figures.
# - Image Captioning (27 points)



###################################################################
###################################################################
## Image Captioning (27 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (A): Data Exploration (12 points)  | 
# ------------------------------------------------------------------

# Question 1 (/4): How many images are in the training set?
image_captioning_a_1: 
- 82783

# Question 2 (/4): To the nearest integer, how many captions are there per image?
image_captioning_a_2: 
- 5

# Question 3 (/4): How many unique tokens are there after splitting captions on words only?
image_captioning_a_3: 
- 45186


# ------------------------------------------------------------------
# | Section (B): Show and Tell (5 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What parts of the CNN were trained?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_b_1: 
 - The top feed forward layer of the CNN

# Question 2 (/1): What was the biggest concern when deciding?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_b_2: 
 - Overfitting

# Question 3 (/1): How was the encoded image representation input into the decoder?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_b_3: 
 - As the first word embedding input

# Question 4 (/1): Which metric did the authors use to determine success?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_b_4: 
 - BLEU

# Question 5 (/1): What beam width is equivalent to one where you select the highest probability word in each decoding step?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_b_5: 
 - 1


# ------------------------------------------------------------------
# | Section (C): Show, Tell and Attend (2 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What is the attention over?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_c_1: 
 - Vectors are extracted from a layer of the CNN with a receptive field (image region contributing to it) smaller than the full image

# Question 2 (/1): What do the figures with highlight shading represent in Figures 2, 3 and 5?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_c_2: 
 - The part of the image contributing to the word currently being decoded


# ------------------------------------------------------------------
# | Section (D): CLIP (8 points)  | 
# ------------------------------------------------------------------

# Question 1 (/2): What is the animal tag you selected?
image_captioning_d_1: sheep

# Question 2 (/2): What is the transportation tag you selected?
image_captioning_d_2: motorcycle

# Question 3 (/2): What is the probability associated with the most likely caption for image 1?
image_captioning_d_3: 0.94

# Question 4 (/2): What is the probability associated with the most likely caption for image 2?
image_captioning_d_4: 0.97
